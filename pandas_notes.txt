df[df.ColumnA == "whatever"]
- returns rows where ColumnA is whatever

df.loc["index", "columna"]
- returns the value matching the given index and given columna

df.describe()
- returns basic stats by column

***************
LOC - filtering rows and coluns by label name
- inclusive for both sides

df.loc[0,:]
- returns first row, all columns (series)

df.loc[0:2, :]
- returns first three rows, all columns (df)

df.loc[:, "columna"]
- returns all rows for columna (series)

df.loc[:, ["columna", "columnb"]]
- returns all rows, column a and b (df)

df.loc[:, ["columna":"columnz"]]
- returns all rows, columns from a to z (df)

df.loc[df.columna == "whatever"]
- returns all rows, where columna is whatever

df.loc[df.columna == "whatever"].columnb
- returns columnb only where columna is whatever (= chained indexing)

df.loc[df.columna == "whatever", "columnb"]
- returns columnb only where columna is whatever (better solution than the one above)

use several where:
df.loc[(df["B"] > 50) & (df["C"] == 900), "A"]

show rows containing substring
df.loc[df.columna.str.contains("mystring")]

select except
df[df["col"].str.contains('this|that')==False]

show duplicate rows – will only show the second duplicate
df["is_duplicate"] = df.duplicated()
df.loc[df.is_duplicate == True]
del df["is_duplicate"]

***************
ILOC - filtering rows and columns by integer position
- exclusive for second value

df.iloc[:, [0,3]]
- returns all rows, and columns 1th and 2nd

df.iloc[:, 0:4]
returns all rows from 1st to 3rd column

***************
IX - filtering rows and columns mixing labels and integer positions
its better not to use this.

df.ix["index label", 0]
- returns first column value for given row

df.ix[1,"columna"]

df.index.name = None

merged = pd.merge(df_new, df_n, left_on='subject_id', right_on='subject_id', how = 'left')

new_header = housing_2000.iloc[0] #grab the first row for the header
housing_2000 = housing_2000[1:] #take the data less the header row

*********
legnth of df
len(df.index)

list of column names: list(my_dataframe.columns.values)

merge(left, right, how='inner', on=None, left_on=user, right_on=Id, left_index=False, right_index=False, sort=True, suffixes=('_x', '_y'), copy=True, indicator=False)

pd.merge(df1, df2, how='inner',left_on='Id', right_on='user', left_index=True, right_index=False, sort=False, copy=False)

pd.merge(user, ert, left_on='user_id', right_on = 'user', how='outer')

df['x'].str.lower()

df = pd.DataFrame(['a|b', 'c|d'])

s = df[0].apply(lambda x: x.split('|'))

df['left'] = s.apply(lambda x: x[0])

df['right'] = s.apply(lambda x: x[1])

df.drop(df[df.ColumnA == "string"].index)

delete rows with missing values in given row:
data.dropna(subset = ['columnA'])
df.dropna(subset = ['column1_name', 'column2_name', 'column3_name'])


groupby several values:
df = df.groupby(['pidx','pidy']).agg({'flag':'first', 'count':'sum'}).reset_index()

normalized:
data["final_norm"] = (data["final"] - data["final"].min()) / (data["final"].max()-data["final"].min())

def rescale(input_array):
    L = numpy.min(input_array)
    H = numpy.max(input_array)
    output_array = (input_array - L) / (H - L)
    return output_array

*********
looking for partial string:
df[df['A'].str.contains("hello")]

ignore NaNs:
df.loc[df.a.str.contains("foo", na=False)]

case insensitive:
gtin[gtin['BRAND_NAME'].str.contains("brand_name_x", case = False)]

list of columns
list(my_dataframe.columns.values)

replace stuff in columns
myfile.columns = myfile.columns.str.replace(' ','_').str.lower()

remove whitespaces from header
shorter.rename(columns=lambda x: x.replace(" ", "_"), inplace=True)

add prefix to columns
myfile = myfile.add_prefix("prefix_")

count values of column
data['columnA'].value_counts()

with index sorted:
data['columnA'].value_counts().sort_index()

convert str to date:
df['col'] = pd.to_datetime(df['col'])

convert float to str
df["id"] = df["id"].apply(str)

to_datetime(shorter['Day'])

groupby sum:
data.groupby(by=['account_ID'])['purchases'].sum()

move columns
df = df[['a', 'y', 'b', 'x']]

count distinict
len(data["columnA"].unique())

show rows with Nans
df[df.isnull().any(axis=1)]

show rows with NaNs in a given column
data.loc[pd.isnull(data.columnA)]

show rows with NOT NaNs in a given column
data.loc[pd.notnull(data.columnA)]

replace nans with sg:
df.column1 = df.column1.fillna('some_string')

rename columns
df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)

change order of columns
data2 = data2[['Profile Id', 'Profile Name', 'eventCategory', 'eventAction', 'eventLabel', 'date', 'deviceCategory', 'channelGrouping', 'UniqueEvents','BRAND_NAME', 'SUB_SECTOR_NAME']]

group data by date (date is in datetime format)
df.resample('M').mean()

delete rows cointain string
df.drop(df[df.ColumnA == "string"].index, inplace = True)

see all columns, width of df
pd.set_option('display.max_columns', 500)

fill column with nan
df["D"] = np.nan
df["C"] = ""

grand total
df[["Jan","Feb","Mar","total"]].sum()


*****************
matrix = pd.read_csv("transposed_matrix.csv", sep = ",", index_col = 0)

matrix.columns
matrix.index.values
for row in matrix.index.values:
    for column in matrix.columns:
        print row, column, matrix.loc[row][column]
*******************
Excel -like features
http://pbpython.com/excel-pandas-comp.html

convert int to datetime
df['DateTime'] = df['date'].apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d'))

groupby month
df.groupby(pd.TimeGrouper(freq='M'))

groupby several columns
group_data = df.groupby(['Alphabet','Words'])['COUNTER'].sum()

count distinct
df.groupby(['col1','col2'])['col3'].nunique().reset_index()

trim
df[0] = df[0].str.strip()

replace name 
nka['szekhely'].replace({'Hódmezovásárhely': 'Hódmezővásárhely'}, inplace=True, regex=True)

unicode miatti rinya:
df.replace({u'Akármi': u'Dr. Akármi'}, inplace=True, regex=True)

print full value_counts:
x = nka_hatarontuli["szekhely_strip"].value_counts()
def print_full(x):
    pd.set_option('display.max_rows', len(x))
    print(x)
    pd.reset_option('display.max_rows')
print_full(x)

dropna here a string is whatever:
df.dropna(subset = ['column_name'])

delete column
del df[colname]

scientific notation format
pd.set_option('display.float_format', lambda x: '%.3f' % x)

df.userid = df.userid.map(lambda x: '{:.0f}'.format(x))

sort by value
result = df.sort(['A', 'B'], ascending=[1, 0])

concat dataframes
concat = pd.concat([df1,df2,df3])

new column with fix value
df['Name'] = 'abc'

if function
data['negative'] =  data['sume'].map(lambda x: 'negative' if x < 0 else 'positive')

replace value based on other condition:
data["company"][data["name"].str.contains("whatever string", na=False, case=False)] = "whatever other string"

merge strings with Nan
df['ColA+ColB'] = df['ColA'].fillna('') + df['ColB'].fillna('')

replace values
df.columnA.replace(['value1', 'value2'], ['new_value1', 'new_value2'], inplace=True)

multi indexing and sorting
#data2.groupby(by=["link_domain"])["link_domain", "engagement_fb"].agg(['mean', 'count']).reset_index().sort_values([("engagement_fb", "mean")], ascending = False).head(20)

give colum names
data = pd.read_csv("path/to/file.txt", sep='\t', header=None)
data.columns = ["Sequence", "Start", "End", "Coverage"]

list_of_what = df["what"].tolist()

check if list is in df:
df[df['A'].([3, 6])]

list not in df:
df[~df.column.isin(mylist)]

for index,row in df.iterrows():
    if a < (row["year_avg"] - row["yerr1"]):
          df.set_value(index,'position',"below")
    elif (a > (row["year_avg"] + row["yerr1"])):
          df.set_value(index,'position',"above")
    else:
          df.set_value(index,'position',"around")

drop duplicates by column
cpv.drop_duplicates(subset='wins_id', keep='last')

reorder columns
frame = frame[['column I want first', 'column I want second'...etc.]]

count co ocurrences
df.groupby(["Group", "Size"]).size()

one liner for tableau format:
pd.melt(df, id_vars=['A'], value_vars=['B', 'C'])

remove new lines
with open("my_file.csv", "rb") as csvfile:
    filtered = (line.replace('\n', ' ') for line in csvfile)
    spamreader = csv.reader(filtered, delimiter=';')
    
    
apply function
def my_function(df):
    if whatever:
        retrun x
    elif whatever:
        return y
    else:
        return x

mydf['new_column'] = mydf.apply(my_function, axis=1)

MySQL
from pandas import DataFrame
from sqlalchemy.engine import create_engine

engine = create_engine("mysql+pymysql://USERNAME:PASSWORD@HOST/DATABASE?charset=utf8mb4")
# dialect+driver://username:password@host:port/database

connection = engine.connect()
connection.execute("SELECT * FROM table WHERE LOWER(name) NOT LIKE '%%whatnot%%';")
df = DataFrame(my_query.fetchall())
df.columns = my_query.keys()
df.head()
connection.close()


calculate time difference between rows
df['delta'] = (df['tvalue']-df['tvalue'].shift()).fillna(0)

calculate time difference between rows by groups
df_sorted = df.sort_values(["session_id", "date2"])
df_sorted['delta'] = df_sorted.groupby('session_id')['date2'].diff()



*************************
plotting
plot
data.groupby(['year']).agg({'id':'count', 'final':'sum'}).plot(kind = 'bar')

plot average line
plt.axhline(stuff, color = "red")

import matplotlib.pyplot as plt
from matplotlib.pyplot import *
import matplotlib.dates as mdates
fig, ax = subplots()

data.groupby([pd.Grouper(key='date', freq='D'), 'airline_sentiment']).size().unstack().plot(figsize=(15,7), \
  color = ['#FF860C', 'grey', '#0080FF'], linewidth = 2, ax = ax)
ax.xaxis.set_major_locator(mdates.DayLocator(interval = 3))
ax.set_ylabel("number of tweets")
ax.spines['top'].set_visible(False) # remove frame
ax.legend(loc='upper left') # legend in upper left corner
plt.xticks(fontsize = 14, rotation=45) # rotate labels and fontsize

colors
https://matplotlib.org/examples/color/named_colors.html

seaborn colormaps
https://gallantlab.github.io/colormaps.html

plots next to each other
fig, axs = plt.subplots(1,2, figsize=(15, 5))
1: ax = search_merged.plot(x="day", y="zero_results_rate", kind="line", color = "black", ax=axs[0])
2: ax = search_merged.plot(x="day", y="zero_results_rate", kind="line", color = "black", ax=axs[1])

rotate labels for both plots
for ax in fig.axes:
    matplotlib.pyplot.sca(ax)
    plt.xticks(rotation=90);

remove y ticks
ax.set_yticklabels([]) # remove y values
ax.tick_params(bottom="off", left="off") # remove ticks


df1 = df.iloc[::-1] # revers df to have it in alphabetical order
my_colors = ["darkorange", "dodgerblue"]  # version 6
ax = df1.plot(kind='barh',stacked=True, figsize=(10, 7), color=my_colors)
ax.set_ylabel('') # remove y axis title
ax.spines['top'].set_visible(False) # remove frame # remove box
ax.spines['bottom'].set_visible(False) # remove frame
ax.spines['left'].set_visible(False) # remove frame
ax.spines['right'].set_visible(False) # remove frame 
ax.axvline(45, color='black', label = "health quangos average") # add lines average
ax.axvline(50.1, color='grey', linestyle = "--", label = "national average")
ax.legend(loc='center',  bbox_to_anchor=(1.1, 0.5)) # legend in upper left corner;
#ax.figure.savefig('../exports/health/institute_gender_v6_with_national_average_for_text.png', bbox_inches='tight', pad_inches=0.1,
#        frameon=None, dpi=400)
;

add labels to bar charts
for rect in ax.patches:
    height = rect.get_height()
    ax.text(rect.get_x() + rect.get_width()/2., height, '%d' % int(height), ha='center', va='bottom')

labels for horizontal bar charts (barh)
for i in ax.patches:
    # get_width pulls left or right; get_y pushes up or down
    ax.text(i.get_width()+.5, i.get_y(), int(i.get_width()), ha='center')

	original line: ax.text(i.get_width()+.3, i.get_y()+.38, \
            str(round((i.get_width()/total)*100, 2))+'%', fontsize=15, color='dimgrey')
            
treemap
import squarify
import matplotlib.pyplot as plt 
squarify.plot(sizes=df['count'], label=df['group'], alpha=.8 )
plt.axis('off')
plt.show()
*************************
